{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Rule_based.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12038e45",
        "outputId": "3ef0fb89-6ea1-4f25-add0-4d85760ecc03"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "id": "12038e45",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2dc751"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Applies some pre-processing on the given text.\n",
        "\n",
        "    Steps :\n",
        "    - Removing punctuation\n",
        "    - Lowering text\n",
        "    \"\"\"\n",
        "    \n",
        "    # remove the characters [\\], ['] and [\"]\n",
        "    text = re.sub(r\"\\\\\", \"\", text)    \n",
        "    text = re.sub(r\"\\'\", \"\", text)    \n",
        "    text = re.sub(r\"\\\"\", \"\", text)    \n",
        "    \n",
        "    # convert text to lowercase\n",
        "    text = text.strip().lower()\n",
        "    \n",
        "    # replace punctuation characters with spaces\n",
        "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((c, \" \") for c in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "\n",
        "    return text"
      ],
      "id": "fb2dc751",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2137aefa"
      },
      "source": [
        "#Importing DataFrame\n",
        "raw_dataframe = pd.read_excel(\"Datasets _ NLP based tagging solution078466f.xlsx\",sheet_name = None)\n",
        "\n",
        "training_data = raw_dataframe['Training Data ']\n",
        "tag_frame = raw_dataframe['Keywords for tagging']\n",
        "\n",
        "Bus_Exp_Tags = tag_frame['Keywords for Business Exception']\n",
        "Bus_Exp_Tags = Bus_Exp_Tags.iloc[0:8]\n",
        "Sof_Exp_Tags = tag_frame['Keywords for System Exception']\n"
      ],
      "id": "2137aefa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac5bdf58"
      },
      "source": [
        "#Preprocessing Data\n",
        "Bus_Exp_Tags.apply(clean_text)\n",
        "Sof_Exp_Tags.apply(clean_text)\n",
        "Bus_Exp_Tags = Bus_Exp_Tags.unique()\n",
        "Sof_Exp_Tags = Sof_Exp_Tags.unique()\n",
        "\n",
        "Train_data = training_data['Exception (input)'].apply(clean_text)\n",
        "Actual_Class = training_data['Exception Category (ouput)']"
      ],
      "id": "ac5bdf58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1d8c2e4"
      },
      "source": [
        "#Removing StopWords\n",
        "Words=[]\n",
        "for line in Train_data:\n",
        "    text_tokens = line.split()\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "    Words.append(tokens_without_sw)\n",
        "\n",
        "Words_BE = []\n",
        "Words_SE = []\n",
        "\n",
        "for line in Bus_Exp_Tags:\n",
        "    text_tokens = line.lower().split()\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "    Words_BE += tokens_without_sw\n",
        "\n",
        "for line in Sof_Exp_Tags:\n",
        "    text_tokens = line.lower().split()\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "    Words_SE += tokens_without_sw\n",
        "    "
      ],
      "id": "a1d8c2e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbda6238"
      },
      "source": [
        "#Assigning weights to tags inversely proportional to their frequency\n",
        "ALL_WORDS = {}\n",
        "BE_WORDS = {}\n",
        "SE_WORDS = {}\n",
        "\n",
        "for line in Words:\n",
        "    for tok in line:\n",
        "        if tok in ALL_WORDS:\n",
        "            ALL_WORDS[tok] += 1\n",
        "        else :\n",
        "            ALL_WORDS[tok] = 1\n",
        "\n",
        "\n",
        "for word in Words_BE:\n",
        "    if word in ALL_WORDS :\n",
        "        BE_WORDS[word] = ALL_WORDS[word]\n",
        "    else:\n",
        "        BE_WORDS[word] = 0\n",
        "        \n",
        "for word in Words_SE:\n",
        "    if word in ALL_WORDS :\n",
        "        SE_WORDS[word] = ALL_WORDS[word]\n",
        "    else:\n",
        "        SE_WORDS[word] = 0\n",
        "max_count_BE = max(BE_WORDS.values())\n",
        "max_count_SE = max(SE_WORDS.values())\n",
        "\n",
        "for word in BE_WORDS:\n",
        "    BE_WORDS[word]=max_count_BE-BE_WORDS[word]\n",
        "    \n",
        "for word in SE_WORDS:\n",
        "    SE_WORDS[word]=max_count_SE-SE_WORDS[word]+1\n"
      ],
      "id": "fbda6238",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87507ebe"
      },
      "source": [
        "#Rule Based Classification\n",
        "Pred_Class = []\n",
        "i=0\n",
        "for line in Words:\n",
        "    count_BE=0\n",
        "    count_SE=0\n",
        "\n",
        "    \n",
        "    for keyword in Words_BE:\n",
        "        if keyword in line:\n",
        "#             count_BE+=1\n",
        "            count_BE+=BE_WORDS[keyword]\n",
        "            \n",
        "    for keyword in Words_SE:\n",
        "        if keyword in line:\n",
        "#             count_SE+=1\n",
        "            count_SE+=SE_WORDS[keyword]\n",
        "    \n",
        "    if count_BE >= count_SE:\n",
        "        Pred_Class.append(\"Business Exception\")\n",
        "    else:\n",
        "        Pred_Class.append(\"System Exception\")"
      ],
      "id": "87507ebe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a61fff35",
        "outputId": "7a57d56b-ba22-477a-cd2e-38f8d6c294b8"
      },
      "source": [
        "#Accuracy\n",
        "count = 0\n",
        "for i in range(0,len(Pred_Class)):\n",
        "    if Pred_Class[i] == Actual_Class[i]:\n",
        "        count+=1\n",
        "print(count*100.0/len(Pred_Class))"
      ],
      "id": "a61fff35",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93.61702127659575\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}